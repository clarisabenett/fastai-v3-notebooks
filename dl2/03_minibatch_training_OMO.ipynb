{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_02 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's setup our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_data() defined in earlier notebooks -> .py\n",
    "x_train,y_train,x_valid,y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 784])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,p = x_train.shape # num images,num pixel values\n",
    "c = (y_train.max() + 1).item() #num classes\n",
    "nh = 50 #num hidden layers (arbitrary-ish)\n",
    "n,p,c #print them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's setup our model\n",
    "## (using pytorch nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Module was imported in earlier notebook\n",
    "class Model(nn.Module): \n",
    "    def __init__(self,n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in,nh),nn.ReLU(),nn.Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(p,nh,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's define a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -->Exploration of how log_softmax() works and cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7497, -1.1975, -1.1989,  0.2238,  1.2403])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5);\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1165)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = x[0].exp()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.1165, 0.3020, 0.3015, 1.2508, 3.4567])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = x.exp()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.4274)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = b.sum(0) #don't keep dim over which you summed\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.4274])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2 = b.sum(0,keepdim=True) #keep dim over which you summed\n",
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2850, 0.0407, 0.0406, 0.1684, 0.4654])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b/c1 #returns the same as b/c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2850, 0.0407, 0.0406, 0.1684, 0.4654])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b/c2 #returns the samne as b/c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1716, -2.2389, -2.1218, -2.2766, -2.3863, -2.3663, -2.5781, -2.6097,\n",
       "         -2.1043, -2.3049],\n",
       "        [-2.1986, -2.3499, -2.1167, -2.2967, -2.3920, -2.3900, -2.3855, -2.4639,\n",
       "         -2.0853, -2.4287],\n",
       "        [-2.2968, -2.3643, -2.0965, -2.2235, -2.3792, -2.3731, -2.3098, -2.4128,\n",
       "         -2.1855, -2.4395]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_preds = log_softmax(pred) #log_softmax predictions\n",
    "sm_preds[:3] #loss landscape for first 3 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3] # actual label of first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3663, 2.1986, 2.3792], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-sm_preds[[0,1,2],y_train[:3]] #loss on correctly predicting first 3 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --> End of Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#me trying to come up with my own nnl() function:\n",
    "def nll(ll,target): return -(ll[range(target.shape[0]),target]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3065, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll(sm_preds,y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify log_softmax()\n",
    "def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(nll(log_softmax(pred),y_train),loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simplification to avoid calculating large numbers \n",
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x-m[:,None]).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare our logsumexp() func to PyTorch .logsumexp()\n",
    "test_near(logsumexp(pred), pred.logsumexp(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use PyTorch version of logsumexp() in our softmax func\n",
    "def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test our new log_softmax()\n",
    "test_near(nll(log_softmax(pred),y_train),loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch implements F.log_softmax and F.nll_loss\n",
    "test_near(F.nll_loss(F.log_softmax(pred,dim=-1),y_train),loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch combines F.log_softmax() and F.nll_loss into F.cross_entropy()\n",
    "test_near(F.cross_entropy(pred,y_train),loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay now we have a loss function called F.cross_entropy(predictions,y_train)\n",
    "### We can now try to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apparently cross_entropy is class that is callable??\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --> Exploration of argmax() amongst other things.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4919, -0.4614,  0.2526])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleep = torch.randn(3)\n",
    "bleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2526)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indMax = bleep.argmax(dim=0); #find index of max value\n",
    "bleep[indMax.item()]#check to make sure it returns the max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "blop = torch.tensor([0,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --> End of Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial accuracy is slightly better than random\n",
    "#We expect this since we haven't trained yet\n",
    "acc =((pred.argmax(dim=1)== y_train).float().sum())/pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#define a function to calculate accuracy of preds\n",
    "def accuracy(out,yb): return (out.argmax(dim=1)==yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(accuracy(pred,y_train),acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get to splitting data into mini-batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64 #batch size has to be a power of 2 or something??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 784])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = x_train[:bs]\n",
    "xb.shape[0] == bs;\n",
    "\n",
    "yb = y_train[:bs]\n",
    "yb.shape[0] == bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preds for a mini-batch\n",
    "pred = model(xb)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3169, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find our loss on this mini-batch\n",
    "loss_func(pred,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1250)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy for mini-batch\n",
    "accuracy(pred,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a learning rate and iterations/input\n",
    "lr = 0.50\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try a training loop:\n",
    "\n",
    "#forward pass\n",
    "for i in range(epochs):\n",
    "    for k in range(n // bs): #NOT SURE WHY HE HAS SOMETHING DIFFERENT\n",
    "        start_k = k * bs\n",
    "        end_k = start_k + bs\n",
    "        xb = x_train[start_k:end_k]\n",
    "        yb = y_train[start_k:end_k]\n",
    "        loss = loss_func(model(xb),yb)\n",
    "        \n",
    "#backward pass\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l,'weight'):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias   -= l.bias.grad * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0381, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I actually achieve 100% accuracy...\n",
    "#but this is on training data!\n",
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using parameters and optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Module was imported in earlier notebook\n",
    "class Model(nn.Module): \n",
    "    def __init__(self,n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in,nh),nn.ReLU(),nn.Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Updated Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I guess we want to rearrange how our model works??\n",
    "#Not sure why yet.\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,n_in,nh,n_out):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "        \n",
    "    def __call__(self, x): return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(p,nh,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of child: l1 -->Layer: Linear(in_features=784, out_features=50, bias=True)\n",
      "Name of child: l2 -->Layer: Linear(in_features=50, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name,l in model.named_children():print(f\"Name of child: {name} -->Layer: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=50, bias=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -->Okay, rewind to understand __setattr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModule():\n",
    "    def __init__(self,n_in,nh,n_out):\n",
    "        self._modules = {}\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "        \n",
    "    def __setattr__(self,k,v):\n",
    "        if not k.startswith('_'): self._modules[k] = v\n",
    "        super().__setattr__(k,v)#need to understand this\n",
    "        \n",
    "    def __repr__(self): return f'{self._modules}'\n",
    "    \n",
    "    def parameters(self):\n",
    "        for l in self._modules.values():\n",
    "            for p in l.parameters(): yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = DummyModule(p,nh,c)\n",
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object DummyModule.parameters at 0x1a28552840>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remember, parameters() is a generator so it doesn't\n",
    "#return a list but rather an iterable thing-like\n",
    "mdl.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([50, 784]),\n",
       " torch.Size([50]),\n",
       " torch.Size([10, 50]),\n",
       " torch.Size([10])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.shape for o in mdl.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --> End of Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the updated model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,n_in,nh,n_out):\n",
    "        super().__init__() #this does equiv of DummyModule()\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "        \n",
    "    def __call__(self, x): return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update fit w/ parameter refactoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for i in range(epochs):\n",
    "        for k in range(n // bs): #NOT SURE WHY HE HAS SOMETHING DIFFERENT\n",
    "            start_k = k * bs\n",
    "            end_k = start_k + bs\n",
    "            xb = x_train[start_k:end_k]\n",
    "            yb = y_train[start_k:end_k]\n",
    "            loss = loss_func(model(xb),yb)\n",
    "\n",
    "    #backward pass\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters(): p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "    #             for l in model.layers:\n",
    "    #                 if hasattr(l,'weight'):\n",
    "    #                     l.weight -= l.weight.grad * lr\n",
    "    #                     l.bias   -= l.bias.grad * lr\n",
    "    #                     l.weight.grad.zero_()\n",
    "    #                     l.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(p,nh,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0392, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay let's just go back to a layer list instead of listing out each layer individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(p,nh), nn.ReLU(), nn.Linear(nh,c)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)                                                           \n",
    "    def __call__(self,x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer_1): ReLU()\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.ModuleList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay so instead of enumerating, pyTorch has an nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self,layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#where exactly in ModuleList, the module_list is initialized\n",
    "#is still quite fuzzy...need to revisit\n",
    "nn.ModuleList??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SequentialModel(layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3124, grad_fn=<NllLossBackward>), tensor(0.0469))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0421, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Sequential does the same thing as Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(p,nh),nn.ReLU(),nn.Linear(nh,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0449, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Sequential??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace our previous manually coded optimization step:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()\n",
    "```\n",
    "\n",
    "and instead use just:\n",
    "\n",
    "```python\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr\n",
    "        \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params: p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(p,nh),nn.ReLU(),nn.Linear(nh,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update fit with optim refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for i in range(epochs):\n",
    "        for k in range(n // bs): #NOT SURE WHY HE HAS SOMETHING DIFFERENT\n",
    "            start_k = k * bs\n",
    "            end_k = start_k + bs\n",
    "            xb = x_train[start_k:end_k]\n",
    "            yb = y_train[start_k:end_k]\n",
    "            loss = loss_func(model(xb),yb)\n",
    "\n",
    "    #backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "#             with torch.no_grad():\n",
    "#                 for p in model.parameters(): p -= p.grad * lr\n",
    "#                 model.zero_grad()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0463, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch already provides this exact functionality in `optim.SGD` (it also handles stuff like momentum, which we'll look at later - except we'll be doing it in a more flexible way!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's import that from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay let's take our two things from before....model and opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(p,nh),nn.ReLU(),nn.Linear(nh,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use optim.SGD...takes params and lr from documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(),lr) #Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take our updated fit function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for i in range(epochs):\n",
    "        for k in range(n // bs): #NOT SURE WHY HE HAS SOMETHING DIFFERENT\n",
    "            start_k = k * bs\n",
    "            end_k = start_k + bs\n",
    "            xb = x_train[start_k:end_k]\n",
    "            yb = y_train[start_k:end_k]\n",
    "            loss = loss_func(model(xb),yb)\n",
    "\n",
    "    #backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0465, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It worked! cool..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Jeremy's sake, we will combine model, opt into a get_model() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(p,nh),nn.ReLU(),nn.Linear(nh,c))\n",
    "    return model, optim.SGD(model.parameters(),lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3242, grad_fn=<NllLossBackward>), tensor(0.1406))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()\n",
    "loss,acc = loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This also worked!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a randomized test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc > 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader!!! ... really hoping to understand this better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clunky to iterate through minibatches of x and y values\n",
    "separately:\n",
    "\n",
    "```python\n",
    "    xb = x_train[start_i:end_i]\n",
    "    yb = y_train[start_i:end_i]\n",
    "```\n",
    "\n",
    "Instead, let's do these two steps together, by introducting a `Dataset` classs:\n",
    "\n",
    "```python\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset():\n",
    "    def __init__(self,x,y): self.x, self.y = x,y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self,i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create datasets and make sure they have the same length as raw data\n",
    "train_ds, valid_ds = Dataset(x_train,y_train),Dataset(x_valid,y_valid)\n",
    "assert len(train_ds) == len(y_train)\n",
    "assert len(valid_ds) == len(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try to grab a batch using Dataset() class\n",
    "xb, yb = train_ds[0:5]\n",
    "assert xb.shape==(5,28*28)\n",
    "assert yb.shape==(5,)\n",
    "xb,yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take our updated fit() function and refactor with Dataset() class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for i in range(epochs):\n",
    "        for k in range(n // bs): #NOT SURE WHY HE HAS SOMETHING DIFFERENT\n",
    "            xb,yb = train_ds[k*bs:k*bs + bs]\n",
    "#             start_k = k * bs\n",
    "#             end_k = start_k + bs\n",
    "#             xb = x_train[start_k:end_k]\n",
    "#             yb = y_train[start_k:end_k]\n",
    "            loss = loss_func(model(xb),yb)\n",
    "\n",
    "    #backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()\n",
    "loss,acc = loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc>0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0180, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, our loop iterated over batches (xb,yb)\n",
    "like this:\n",
    "\n",
    "```python\n",
    "    for i in range((n-1)//bs + 1):\n",
    "        xb,yb = train_ds[i*bs:i*bs+bs]\n",
    "```\n",
    "\n",
    "Let's make our loop much cleaner, using a data loader:\n",
    "\n",
    "```python\n",
    "    for xb,yb in train_dl:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, bs): self.ds, self.bs = ds,bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0,len(self.ds),bs): yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds,bs)\n",
    "valid_dl = DataLoader(valid_ds,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "         1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "         9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = next(iter(train_dl));\n",
    "xb,yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take our updated fit() function and refactor with DataLoader() class..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for i in range(epochs):\n",
    "        for xb,yb in train_dl:\n",
    "#         for k in range(n // bs): #NOT SURE WHY HE HAS SOMETHING DIFFERENT\n",
    "#             xb,yb = train_ds[k*bs:k*bs + bs]\n",
    "            loss = loss_func(model(xb),yb)\n",
    "\n",
    "    #backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()\n",
    "loss,acc = loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc > 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1751, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay let's cleanup our fit() function a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for i in range(epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred,yb)\n",
    "    #backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn't be randomized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -->Exploration of torch.randperm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7,  0, 17, 16, 18,  3, 13,  8,  6,  4,  1,  9,  2, 14,  5, 10, 15, 19,\n",
       "        12, 11])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shuffling my indices (inputs)\n",
    "idxs =torch.randperm(20)\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7,  0, 17, 16])\n",
      "tensor([18,  3, 13,  8])\n",
      "tensor([6, 4, 1, 9])\n",
      "tensor([ 2, 14,  5, 10])\n",
      "tensor([15, 19, 12, 11])\n"
     ]
    }
   ],
   "source": [
    "#producing each batch in consecutive order of the shuffled indices\n",
    "for i in range(0,20,4): print(idxs[i:i+4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -->End of exporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a Sampler class that returns the indices\n",
    "#of the inputs that you want to grab from dataset\n",
    "class Sampler():\n",
    "    def __init__(self,ds,bs,shuffle=False):\n",
    "        self.n = len(ds)\n",
    "        self.bs = bs\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n)if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a smaller dataset\n",
    "#you have to use *train_ds so python knows\n",
    "#there are more than one argument contained\n",
    "small_ds = Dataset(*train_ds[:10])\n",
    "len(small_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sampler(small_ds, 3, False)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#different from last time\n",
    "s = Sampler(small_ds, 3, False)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#different from last times\n",
    "s = Sampler(small_ds, 3, False)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of zip and torch.stack and other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = torch.randn(3)\n",
    "there = torch.randn(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3201, -0.7348,  1.2825])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7150, -1.0039, -0.6368])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor(-0.3201), tensor(-0.7150)),\n",
       " (tensor(-0.7348), tensor(-1.0039)),\n",
       " (tensor(1.2825), tensor(-0.6368))]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob = zip(hello,there)\n",
    "bob = [k for k in bob]\n",
    "bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello,there = zip(*bob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.3201), tensor(-0.7348), tensor(1.2825))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.7150), tensor(-1.0039), tensor(-0.6368))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jim = next(iter(s))\n",
    "jim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0117,\n",
       "          0.0703, 0.0703, 0.0703, 0.4922, 0.5312, 0.6836, 0.1016, 0.6484, 0.9961,\n",
       "          0.9648, 0.4961, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1172, 0.1406, 0.3672, 0.6016,\n",
       "          0.6641, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.8789, 0.6719, 0.9883,\n",
       "          0.9453, 0.7617, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1914, 0.9297, 0.9883, 0.9883,\n",
       "          0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9805, 0.3633, 0.3203,\n",
       "          0.3203, 0.2188, 0.1523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0703, 0.8555, 0.9883,\n",
       "          0.9883, 0.9883, 0.9883, 0.9883, 0.7734, 0.7109, 0.9648, 0.9414, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3125,\n",
       "          0.6094, 0.4180, 0.9883, 0.9883, 0.8008, 0.0430, 0.0000, 0.1680, 0.6016,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0547, 0.0039, 0.6016, 0.9883, 0.3516, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.5430, 0.9883, 0.7422, 0.0078, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0430, 0.7422, 0.9883, 0.2734,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1367, 0.9414,\n",
       "          0.8789, 0.6250, 0.4219, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.3164, 0.9375, 0.9883, 0.9883, 0.4648, 0.0977, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.1758, 0.7266, 0.9883, 0.9883, 0.5859, 0.1055, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0625, 0.3633, 0.9844, 0.9883, 0.7305,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9727, 0.9883,\n",
       "          0.9727, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1797, 0.5078, 0.7148, 0.9883,\n",
       "          0.9883, 0.8086, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1523, 0.5781, 0.8945, 0.9883, 0.9883,\n",
       "          0.9883, 0.9766, 0.7109, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0938, 0.4453, 0.8633, 0.9883, 0.9883, 0.9883,\n",
       "          0.9883, 0.7852, 0.3047, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0898, 0.2578, 0.8320, 0.9883, 0.9883, 0.9883, 0.9883,\n",
       "          0.7734, 0.3164, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0703, 0.6680, 0.8555, 0.9883, 0.9883, 0.9883, 0.9883, 0.7617,\n",
       "          0.3125, 0.0352, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.2148, 0.6719, 0.8828, 0.9883, 0.9883, 0.9883, 0.9883, 0.9531, 0.5195,\n",
       "          0.0430, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.5312, 0.9883, 0.9883, 0.9883, 0.8281, 0.5273, 0.5156, 0.0625,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000]), tensor(5)),\n",
       " (tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.1992, 0.6211, 0.9883, 0.6211, 0.1953, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.1875, 0.9297, 0.9844, 0.9844, 0.9844, 0.9258, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.2109, 0.8867, 0.9883, 0.9844, 0.9336, 0.9102, 0.9844, 0.2227,\n",
       "          0.0234, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0391, 0.2344, 0.8750, 0.9844, 0.9883, 0.9844, 0.7891, 0.3281, 0.9844,\n",
       "          0.9883, 0.4766, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.6367, 0.9844, 0.9844, 0.9844, 0.9883, 0.9844, 0.9844, 0.3750,\n",
       "          0.7383, 0.9883, 0.6523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.1992, 0.9297, 0.9883, 0.9883, 0.7422, 0.4453, 0.9883, 0.8906,\n",
       "          0.1836, 0.3086, 0.9961, 0.6562, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.1875, 0.9297, 0.9844, 0.9844, 0.6992, 0.0469, 0.2930, 0.4727,\n",
       "          0.0820, 0.0000, 0.0000, 0.9883, 0.9492, 0.1953, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.1484, 0.6445, 0.9883, 0.9102, 0.8125, 0.3281, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.9883, 0.9844, 0.6445, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0273, 0.6953, 0.9844, 0.9375, 0.2773, 0.0742, 0.1094, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9883, 0.9844, 0.7617, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.2227, 0.9844, 0.9844, 0.2461, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9883, 0.9844, 0.7617,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.7734, 0.9883, 0.7422, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9961, 0.9883,\n",
       "          0.7656, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.2969, 0.9609, 0.9844, 0.4375, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9883,\n",
       "          0.9844, 0.5781, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.3320, 0.9844, 0.8984, 0.0977, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0273, 0.5273,\n",
       "          0.9883, 0.7266, 0.0469, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3320, 0.9844, 0.8711, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0273, 0.5117,\n",
       "          0.9844, 0.8789, 0.2773, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3320, 0.9844, 0.5664,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1875, 0.6445,\n",
       "          0.9844, 0.6758, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3359, 0.9883,\n",
       "          0.8789, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4453, 0.9297,\n",
       "          0.9883, 0.6328, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3320,\n",
       "          0.9844, 0.9727, 0.5703, 0.1875, 0.1133, 0.3320, 0.6953, 0.8789, 0.9883,\n",
       "          0.8711, 0.6523, 0.2188, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.3320, 0.9844, 0.9844, 0.9844, 0.8945, 0.8398, 0.9844, 0.9844, 0.9844,\n",
       "          0.7656, 0.5078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.1094, 0.7773, 0.9844, 0.9844, 0.9883, 0.9844, 0.9844, 0.9102,\n",
       "          0.5664, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0977, 0.5000, 0.9844, 0.9883, 0.9844, 0.5508,\n",
       "          0.1445, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000]),\n",
       "  tensor(0)),\n",
       " (tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2617, 0.9062,\n",
       "          0.1523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.2422, 0.3164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4688,\n",
       "          0.7031, 0.1523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.4922, 0.6367, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078,\n",
       "          0.5977, 0.8203, 0.1562, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.8594, 0.6367, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.1055, 0.9922, 0.6328, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.8672, 0.6367, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.7148, 0.9922, 0.4883, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1797, 0.9570, 0.6367, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.7734, 0.9922, 0.2188, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4688, 0.9922, 0.6367, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0898, 0.9023, 0.9922, 0.1133, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6211, 0.9922, 0.4688,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.6367, 0.9922, 0.8438, 0.0625, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6211, 0.9922,\n",
       "          0.2617, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0547, 0.3359, 0.6953, 0.9688, 0.9922, 0.3555, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6211,\n",
       "          0.9922, 0.3320, 0.0000, 0.0000, 0.0000, 0.1836, 0.1914, 0.4531, 0.5625,\n",
       "          0.5859, 0.9414, 0.9492, 0.9141, 0.6992, 0.9414, 0.9844, 0.1562, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.5859, 0.9883, 0.9258, 0.8086, 0.8086, 0.8086, 0.9883, 0.9922, 0.9766,\n",
       "          0.9375, 0.7734, 0.5586, 0.3555, 0.1094, 0.0195, 0.9102, 0.9766, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.4648, 0.6914, 0.6914, 0.6914, 0.6914, 0.6914, 0.3828,\n",
       "          0.2188, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3984, 0.9922, 0.8594,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6602, 0.9922,\n",
       "          0.5352, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6602,\n",
       "          0.9922, 0.2227, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.6602, 0.9922, 0.2227, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.6602, 0.9961, 0.3672, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.6602, 0.9922, 0.3750, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.6602, 0.9922, 0.5977, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.6602, 0.9961, 0.5977, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3750, 0.9922, 0.5977, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000]), tensor(4))]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "karen = [small_ds[ii] for ii in jim]\n",
    "karen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "janice = small_ds[jim];\n",
    "cc,dd =janice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa,bb = zip(*karen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's check to see that I don't need collate()\n",
    "#by comparing my solution: ds[Sampler(i)]\n",
    "#versus: collate([ds[i] for i in Sampler(i)])\n",
    "torch.all(torch.eq(torch.stack(aa),cc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hey! I found an easier way to implement random sampler!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -->End of Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay let's try to re-do our DataLoader() class using random Sampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler): self.ds, self.sampler = ds,sampler\n",
    "    def __iter__(self):\n",
    "        for ss in self.sampler: yield self.ds[ss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds,bs,shuffle=True)\n",
    "valid_samp = Sampler(valid_ds,bs,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds,train_samp)\n",
    "valid_dl = DataLoader(valid_ds,valid_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey I got different results for each xb[0]! coolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1149, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "fit()\n",
    "\n",
    "loss,acc = loss_func(model(xb),yb),accuracy(model(xb),yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    xs,ys = zip(*b)\n",
    "    return torch.stack(xs),torch.stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds,bs,sampler=RandomSampler(train_ds),collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds,bs,sampler=SequentialSampler(valid_ds),collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1337, grad_fn=<NllLossBackward>), tensor(0.9531))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "fit()\n",
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds,bs,shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds,bs,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0840, grad_fn=<NllLossBackward>), tensor(0.9531))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "fit()\n",
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so now we want to have a validation run through without optimization..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is our most up-to-date fit() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for i in range(epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred,yb)\n",
    "    #backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our fit function take in arguments for all the stuff it needs so it isn't dependent on things that are set in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, train_dl, valid_dl, model, loss_func, aopt):\n",
    "    for i in range(epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred,yb)\n",
    "    #backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay next we are going to try to figure out what our validation dataset should be doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be honest, I think it is a bit weird to calculate loss on the validation set AS I am training vs AFTER I  am training...I mean I guess it is a way to stop our learning if the model is overfitting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, train_dl, valid_dl, model, loss_func, opt, accuracy):\n",
    "    for i in range(epochs): #number of iterations of training and validation metrics calculated        \n",
    "        #training forward pass\n",
    "        for xb,yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred,yb)\n",
    "        #backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        #validation forward pass   \n",
    "        with torch.no_grad():\n",
    "            total_loss, total_acc = 0.,0.\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                total_loss += loss_func(pred,yb)\n",
    "                total_acc  += accuracy(pred,yb)\n",
    "            nb = len(valid_dl) #number of equally-sized mini-batches\n",
    "            print(f\"epoch: {i} | valid_loss: {total_loss / nb} | valid_acc: {total_acc / nb}\")\n",
    "    return total_loss/nb, total_acc/nb        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeremy wants to be fancy and make a function to take in train_ds, valid_ds and pop our dataloaders out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dls(train_ds, valid_ds, bs,**kwargs):\n",
    "    return DataLoader(train_ds, batch_size=bs,shuffle=True,**kwargs), DataLoader(valid_ds, batch_size=2*bs,shuffle=False,**kwargs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | valid_loss: 0.288716584444046 | valid_acc: 0.9099090099334717\n",
      "epoch: 1 | valid_loss: 0.27747538685798645 | valid_acc: 0.9209849834442139\n",
      "epoch: 2 | valid_loss: 0.11218695342540741 | valid_acc: 0.9671677350997925\n",
      "epoch: 3 | valid_loss: 0.2457146942615509 | valid_acc: 0.9271162748336792\n",
      "epoch: 4 | valid_loss: 0.11574181169271469 | valid_acc: 0.9660798907279968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.1157), tensor(0.9661))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl,valid_dl = get_dls(train_ds,valid_ds,bs)\n",
    "model,opt = get_model()\n",
    "loss,acc = fit(5,train_dl,valid_dl,model,loss_func,opt,accuracy)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc>0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hey I did it!!! I made a model, fit it, and confirmed it using validation data!! Whoohooo....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 03_minibatch_training_OMO.ipynb to exp/nb_03.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 03_minibatch_training_OMO.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
