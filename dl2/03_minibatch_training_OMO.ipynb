{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_02_OMO import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's setup our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_data() defined in earlier notebooks -> .py\n",
    "x_train,y_train,x_valid,y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 784])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,p = x_train.shape # num images,num pixel values\n",
    "c = (y_train.max() + 1).item() #num classes\n",
    "nh = 50 #num hidden layers (arbitrary-ish)\n",
    "n,p,c #print them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's setup our model\n",
    "## (using pytorch nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Module was imported in earlier notebook\n",
    "class Model(nn.Module): \n",
    "    def __init__(self,n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in,nh),nn.ReLU(),nn.Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(p,nh,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's define a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -->Exploration of how log_softmax() works and cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9393,  0.8389, -1.6511, -0.8329, -0.5639])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5);\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.9539)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = x[0].exp()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.9539, 2.3139, 0.1918, 0.4348, 0.5690])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = x.exp()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4635)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = b.sum(0) #don't keep dim over which you summed\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.4635])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2 = b.sum(0,keepdim=True) #keep dim over which you summed\n",
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6646, 0.2211, 0.0183, 0.0416, 0.0544])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b/c1 #returns the same as b/c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6646, 0.2211, 0.0183, 0.0416, 0.0544])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b/c2 #returns the samne as b/c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3231, -2.3444, -2.2009, -2.4079, -2.0819, -2.3267, -2.3119, -2.4617,\n",
       "         -2.3113, -2.3066],\n",
       "        [-2.3280, -2.2919, -2.2343, -2.4106, -2.0621, -2.3660, -2.3699, -2.3684,\n",
       "         -2.3479, -2.2940],\n",
       "        [-2.3877, -2.2460, -2.1792, -2.4336, -2.1130, -2.2921, -2.3700, -2.4457,\n",
       "         -2.3729, -2.2423]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_preds = log_softmax(pred) #log_softmax predictions\n",
    "sm_preds[:3] #loss landscape for first 3 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3] # actual label of first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3267, 2.3280, 2.1130], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-sm_preds[[0,1,2],y_train[:3]] #loss on correctly predicting first 3 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --> End of Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#me trying to come up with my own nnl() function:\n",
    "def nll(ll,target): return -(ll[range(target.shape[0]),target]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3040, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll(sm_preds,y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify log_softmax()\n",
    "def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(nll(log_softmax(pred),y_train),loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simplification to avoid calculating large numbers \n",
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x-m[:,None]).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare our logsumexp() func to PyTorch .logsumexp()\n",
    "test_near(logsumexp(pred), pred.logsumexp(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use PyTorch version of logsumexp() in our softmax func\n",
    "def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test our new log_softmax()\n",
    "test_near(nll(log_softmax(pred),y_train),loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch implements F.log_softmax and F.nll_loss\n",
    "test_near(F.nll_loss(F.log_softmax(pred,dim=-1),y_train),loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch combines F.log_softmax() and F.nll_loss into F.cross_entropy()\n",
    "test_near(F.cross_entropy(pred,y_train),loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay now we have a loss function called F.cross_entropy(predictions,y_train)\n",
    "### We can now try to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apparently cross_entropy is class that is callable??\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --> Exploration of argmax() amongst other things.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2581,  0.1415,  0.2046])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleep = torch.randn(3)\n",
    "bleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2046)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indMax = bleep.argmax(dim=0); #find index of max value\n",
    "bleep[indMax.item()]#check to make sure it returns the max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "blop = torch.tensor([0,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --> End of Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial accuracy is slightly better than random\n",
    "#We expect this since we haven't trained yet\n",
    "acc =((pred.argmax(dim=1)== y_train).float().sum())/pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#define a function to calculate accuracy of preds\n",
    "def accuracy(out,yb): return (out.argmax(dim=1)==yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(accuracy(pred,y_train),acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get to splitting data into mini-batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64 #batch size has to be a power of 2 or something??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 784])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = x_train[:bs]\n",
    "xb.shape[0] == bs;\n",
    "\n",
    "yb = y_train[:bs]\n",
    "yb.shape[0] == bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preds for a mini-batch\n",
    "pred = model(xb)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3015, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find our loss on this mini-batch\n",
    "loss_func(pred,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0938)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy for mini-batch\n",
    "accuracy(pred,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a learning rate and iterations/input\n",
    "lr = 0.50\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try a training loop:\n",
    "\n",
    "#forward pass\n",
    "for i in range(epochs):\n",
    "    for k in range(n // bs): #NOT SURE WHY HE HAS SOMETHING DIFFERENT\n",
    "        start_k = k * bs\n",
    "        end_k = start_k + bs\n",
    "        xb = x_train[start_k:end_k]\n",
    "        yb = y_train[start_k:end_k]\n",
    "        loss = loss_func(model(xb),yb)\n",
    "        \n",
    "#backward pass\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l,'weight'):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias   -= l.bias.grad * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0421, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I actually achieve 100% accuracy...\n",
    "#but this is on training data!\n",
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using parameters and optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Module was imported in earlier notebook\n",
    "class Model(nn.Module): \n",
    "    def __init__(self,n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in,nh),nn.ReLU(),nn.Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Updated Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I guess we want to rearrange how our model works??\n",
    "#Not sure why yet.\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,n_in,nh,n_out):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "        \n",
    "    def __call__(self, x): return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(p,nh,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of child: l1 -->Layer: Linear(in_features=784, out_features=50, bias=True)\n",
      "Name of child: l2 -->Layer: Linear(in_features=50, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name,l in model.named_children():print(f\"Name of child: {name} -->Layer: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=50, bias=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -->Okay, rewind to understand __setattr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModule():\n",
    "    def __init__(self,n_in,nh,n_out):\n",
    "        self._modules = {}\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "        \n",
    "    def __setattr__(self,k,v):\n",
    "        if not k.startswith('_'): self._modules[k] = v\n",
    "        super().__setattr__(k,v)#need to understand this\n",
    "        \n",
    "    def __repr__(self): return f'{self._modules}'\n",
    "    \n",
    "    def parameters(self):\n",
    "        for l in self._modules.values():\n",
    "            for p in l.parameters(): yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = DummyModule(p,nh,c)\n",
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object DummyModule.parameters at 0x1a2630a750>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remember, parameters() is a generator so it doesn't\n",
    "#return a list but rather an iterable thing-like\n",
    "mdl.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([50, 784]),\n",
       " torch.Size([50]),\n",
       " torch.Size([10, 50]),\n",
       " torch.Size([10])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.shape for o in mdl.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --> End of Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the updated model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,n_in,nh,n_out):\n",
    "        super().__init__() #this does equiv of DummyModule()\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "        \n",
    "    def __call__(self, x): return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update fit w/ parameter refactoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for i in range(epochs):\n",
    "        for k in range(n // bs): #NOT SURE WHY HE HAS SOMETHING DIFFERENT\n",
    "            start_k = k * bs\n",
    "            end_k = start_k + bs\n",
    "            xb = x_train[start_k:end_k]\n",
    "            yb = y_train[start_k:end_k]\n",
    "            loss = loss_func(model(xb),yb)\n",
    "\n",
    "    #backward pass\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters(): p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "    #             for l in model.layers:\n",
    "    #                 if hasattr(l,'weight'):\n",
    "    #                     l.weight -= l.weight.grad * lr\n",
    "    #                     l.bias   -= l.bias.grad * lr\n",
    "    #                     l.weight.grad.zero_()\n",
    "    #                     l.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(p,nh,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0505, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay let's just go back to a layer list instead of listing out each layer individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(p,nh), nn.ReLU(), nn.Linear(nh,c)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)                                                           \n",
    "    def __call__(self,x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer_1): ReLU()\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.ModuleList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay so instead of enumerating, pyTorch has an nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self,layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#where exactly in ModuleList, the module_list is initialized\n",
    "#is still quite fuzzy...need to revisit\n",
    "nn.ModuleList??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SequentialModel(layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3348, grad_fn=<NllLossBackward>), tensor(0.0781))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0403, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Sequential does the same thing as Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(p,nh),nn.ReLU(),nn.Linear(nh,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0419, grad_fn=<NllLossBackward>), tensor(0.9844))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Sequential??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace our previous manually coded optimization step:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()\n",
    "```\n",
    "\n",
    "and instead use just:\n",
    "\n",
    "```python\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr\n",
    "        \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params: p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(p,nh),nn.ReLU(),nn.Linear(nh,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update fit with optim refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for i in range(epochs):\n",
    "        for k in range(n // bs): #NOT SURE WHY HE HAS SOMETHING DIFFERENT\n",
    "            start_k = k * bs\n",
    "            end_k = start_k + bs\n",
    "            xb = x_train[start_k:end_k]\n",
    "            yb = y_train[start_k:end_k]\n",
    "            loss = loss_func(model(xb),yb)\n",
    "\n",
    "    #backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "#             with torch.no_grad():\n",
    "#                 for p in model.parameters(): p -= p.grad * lr\n",
    "#                 model.zero_grad()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0384, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch already provides this exact functionality in `optim.SGD` (it also handles stuff like momentum, which we'll look at later - except we'll be doing it in a more flexible way!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's import that from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay let's take our two things from before....model and opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(p,nh),nn.ReLU(),nn.Linear(nh,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use optim.SGD...takes params and lr from documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(),lr) #Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take our updated fit function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for i in range(epochs):\n",
    "        for k in range(n // bs): #NOT SURE WHY HE HAS SOMETHING DIFFERENT\n",
    "            start_k = k * bs\n",
    "            end_k = start_k + bs\n",
    "            xb = x_train[start_k:end_k]\n",
    "            yb = y_train[start_k:end_k]\n",
    "            loss = loss_func(model(xb),yb)\n",
    "\n",
    "    #backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0330, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It worked! cool..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Jeremy's sake, we will combine model, opt into a get_model() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(p,nh),nn.ReLU(),nn.Linear(nh,c))\n",
    "    return model, optim.SGD(model.parameters(),lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.2904, grad_fn=<NllLossBackward>), tensor(0.1094))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()\n",
    "loss,acc = loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This also worked!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a randomized test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc > 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader!!! ... really hoping to understand this better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clunky to iterate through minibatches of x and y values\n",
    "separately:\n",
    "\n",
    "```python\n",
    "    xb = x_train[start_i:end_i]\n",
    "    yb = y_train[start_i:end_i]\n",
    "```\n",
    "\n",
    "Instead, let's do these two steps together, by introducting a `Dataset` classs:\n",
    "\n",
    "```python\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset():\n",
    "    def __init__(self,x,y): self.x, self.y = x,y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self,i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create datasets and make sure they have the same length as raw data\n",
    "train_ds, valid_ds = Dataset(x_train,y_train),Dataset(x_valid,y_valid)\n",
    "assert len(train_ds) == len(y_train)\n",
    "assert len(valid_ds) == len(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try to grab a batch using Dataset() class\n",
    "xb, yb = train_ds[0:5]\n",
    "assert xb.shape==(5,28*28)\n",
    "assert yb.shape==(5,)\n",
    "xb,yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take our updated fit() function and refactor with Dataset() class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for i in range(epochs):\n",
    "        for k in range(n // bs): #NOT SURE WHY HE HAS SOMETHING DIFFERENT\n",
    "            xb,yb = train_ds[k*bs:k*bs + bs]\n",
    "#             start_k = k * bs\n",
    "#             end_k = start_k + bs\n",
    "#             xb = x_train[start_k:end_k]\n",
    "#             yb = y_train[start_k:end_k]\n",
    "            loss = loss_func(model(xb),yb)\n",
    "\n",
    "    #backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()\n",
    "loss,acc = loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc>0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0253, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, our loop iterated over batches (xb,yb)\n",
    "like this:\n",
    "\n",
    "```python\n",
    "    for i in range((n-1)//bs + 1):\n",
    "        xb,yb = train_ds[i*bs:i*bs+bs]\n",
    "```\n",
    "\n",
    "Let's make our loop much cleaner, using a data loader:\n",
    "\n",
    "```python\n",
    "    for xb,yb in train_dl:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, bs): self.ds, self.bs = ds,bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0,len(self.ds),bs): yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds,bs)\n",
    "valid_dl = DataLoader(valid_ds,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "         1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "         9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = next(iter(train_dl));\n",
    "xb,yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take our updated fit() function and refactor with DataLoader() class..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for i in range(epochs):\n",
    "        for xb,yb in train_dl:\n",
    "#         for k in range(n // bs): #NOT SURE WHY HE HAS SOMETHING DIFFERENT\n",
    "#             xb,yb = train_ds[k*bs:k*bs + bs]\n",
    "            loss = loss_func(model(xb),yb)\n",
    "\n",
    "    #backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()\n",
    "loss,acc = loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc > 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1067, grad_fn=<NllLossBackward>), tensor(0.9531))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay let's cleanup our fit() function a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for i in range(epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred,yb)\n",
    "    #backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn't be randomized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -->Exploration of torch.randperm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  6, 13,  0, 17,  2, 16,  4,  5, 19, 15, 12, 14,  9, 11, 18,  7,  1,\n",
       "         8, 10])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shuffling my indices (inputs)\n",
    "idxs =torch.randperm(20)\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3,  6, 13,  0])\n",
      "tensor([17,  2, 16,  4])\n",
      "tensor([ 5, 19, 15, 12])\n",
      "tensor([14,  9, 11, 18])\n",
      "tensor([ 7,  1,  8, 10])\n"
     ]
    }
   ],
   "source": [
    "#producing each batch in consecutive order of the shuffled indices\n",
    "for i in range(0,20,4): print(idxs[i:i+4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -->End of exporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a Sampler class that returns the indices\n",
    "#of the inputs that you want to grab from dataset\n",
    "class Sampler():\n",
    "    def __init__(self,ds,bs,shuffle=False):\n",
    "        self.n = len(ds)\n",
    "        self.bs = bs\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n)if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a smaller dataset\n",
    "#you have to use *train_ds so python knows\n",
    "#there are more than one argument contained\n",
    "small_ds = Dataset(*train_ds[:10])\n",
    "len(small_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sampler(small_ds, 3, False)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#different from last time\n",
    "s = Sampler(small_ds, 3, False)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#different from last times\n",
    "s = Sampler(small_ds, 3, False)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of zip and torch.stack and other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = torch.randn(3)\n",
    "there = torch.randn(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4827, -0.6664, -0.6159])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6170, -1.3656, -1.0676])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor(0.4827), tensor(-1.6170)),\n",
       " (tensor(-0.6664), tensor(-1.3656)),\n",
       " (tensor(-0.6159), tensor(-1.0676))]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob = zip(hello,there)\n",
    "bob = [k for k in bob]\n",
    "bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello,there = zip(*bob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4827), tensor(-0.6664), tensor(-0.6159))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.6170), tensor(-1.3656), tensor(-1.0676))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jim = next(iter(s))\n",
    "jim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0117,\n",
       "          0.0703, 0.0703, 0.0703, 0.4922, 0.5312, 0.6836, 0.1016, 0.6484, 0.9961,\n",
       "          0.9648, 0.4961, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1172, 0.1406, 0.3672, 0.6016,\n",
       "          0.6641, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.8789, 0.6719, 0.9883,\n",
       "          0.9453, 0.7617, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1914, 0.9297, 0.9883, 0.9883,\n",
       "          0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9805, 0.3633, 0.3203,\n",
       "          0.3203, 0.2188, 0.1523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0703, 0.8555, 0.9883,\n",
       "          0.9883, 0.9883, 0.9883, 0.9883, 0.7734, 0.7109, 0.9648, 0.9414, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3125,\n",
       "          0.6094, 0.4180, 0.9883, 0.9883, 0.8008, 0.0430, 0.0000, 0.1680, 0.6016,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0547, 0.0039, 0.6016, 0.9883, 0.3516, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.5430, 0.9883, 0.7422, 0.0078, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0430, 0.7422, 0.9883, 0.2734,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1367, 0.9414,\n",
       "          0.8789, 0.6250, 0.4219, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.3164, 0.9375, 0.9883, 0.9883, 0.4648, 0.0977, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.1758, 0.7266, 0.9883, 0.9883, 0.5859, 0.1055, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0625, 0.3633, 0.9844, 0.9883, 0.7305,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9727, 0.9883,\n",
       "          0.9727, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1797, 0.5078, 0.7148, 0.9883,\n",
       "          0.9883, 0.8086, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1523, 0.5781, 0.8945, 0.9883, 0.9883,\n",
       "          0.9883, 0.9766, 0.7109, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0938, 0.4453, 0.8633, 0.9883, 0.9883, 0.9883,\n",
       "          0.9883, 0.7852, 0.3047, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0898, 0.2578, 0.8320, 0.9883, 0.9883, 0.9883, 0.9883,\n",
       "          0.7734, 0.3164, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0703, 0.6680, 0.8555, 0.9883, 0.9883, 0.9883, 0.9883, 0.7617,\n",
       "          0.3125, 0.0352, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.2148, 0.6719, 0.8828, 0.9883, 0.9883, 0.9883, 0.9883, 0.9531, 0.5195,\n",
       "          0.0430, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.5312, 0.9883, 0.9883, 0.9883, 0.8281, 0.5273, 0.5156, 0.0625,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000]), tensor(5)),\n",
       " (tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.1992, 0.6211, 0.9883, 0.6211, 0.1953, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.1875, 0.9297, 0.9844, 0.9844, 0.9844, 0.9258, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.2109, 0.8867, 0.9883, 0.9844, 0.9336, 0.9102, 0.9844, 0.2227,\n",
       "          0.0234, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0391, 0.2344, 0.8750, 0.9844, 0.9883, 0.9844, 0.7891, 0.3281, 0.9844,\n",
       "          0.9883, 0.4766, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.6367, 0.9844, 0.9844, 0.9844, 0.9883, 0.9844, 0.9844, 0.3750,\n",
       "          0.7383, 0.9883, 0.6523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.1992, 0.9297, 0.9883, 0.9883, 0.7422, 0.4453, 0.9883, 0.8906,\n",
       "          0.1836, 0.3086, 0.9961, 0.6562, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.1875, 0.9297, 0.9844, 0.9844, 0.6992, 0.0469, 0.2930, 0.4727,\n",
       "          0.0820, 0.0000, 0.0000, 0.9883, 0.9492, 0.1953, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.1484, 0.6445, 0.9883, 0.9102, 0.8125, 0.3281, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.9883, 0.9844, 0.6445, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0273, 0.6953, 0.9844, 0.9375, 0.2773, 0.0742, 0.1094, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9883, 0.9844, 0.7617, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.2227, 0.9844, 0.9844, 0.2461, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9883, 0.9844, 0.7617,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.7734, 0.9883, 0.7422, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9961, 0.9883,\n",
       "          0.7656, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.2969, 0.9609, 0.9844, 0.4375, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9883,\n",
       "          0.9844, 0.5781, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.3320, 0.9844, 0.8984, 0.0977, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0273, 0.5273,\n",
       "          0.9883, 0.7266, 0.0469, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3320, 0.9844, 0.8711, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0273, 0.5117,\n",
       "          0.9844, 0.8789, 0.2773, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3320, 0.9844, 0.5664,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1875, 0.6445,\n",
       "          0.9844, 0.6758, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3359, 0.9883,\n",
       "          0.8789, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4453, 0.9297,\n",
       "          0.9883, 0.6328, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3320,\n",
       "          0.9844, 0.9727, 0.5703, 0.1875, 0.1133, 0.3320, 0.6953, 0.8789, 0.9883,\n",
       "          0.8711, 0.6523, 0.2188, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.3320, 0.9844, 0.9844, 0.9844, 0.8945, 0.8398, 0.9844, 0.9844, 0.9844,\n",
       "          0.7656, 0.5078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.1094, 0.7773, 0.9844, 0.9844, 0.9883, 0.9844, 0.9844, 0.9102,\n",
       "          0.5664, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0977, 0.5000, 0.9844, 0.9883, 0.9844, 0.5508,\n",
       "          0.1445, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000]),\n",
       "  tensor(0)),\n",
       " (tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2617, 0.9062,\n",
       "          0.1523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.2422, 0.3164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4688,\n",
       "          0.7031, 0.1523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.4922, 0.6367, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078,\n",
       "          0.5977, 0.8203, 0.1562, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.8594, 0.6367, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.1055, 0.9922, 0.6328, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.8672, 0.6367, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.7148, 0.9922, 0.4883, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1797, 0.9570, 0.6367, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.7734, 0.9922, 0.2188, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4688, 0.9922, 0.6367, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0898, 0.9023, 0.9922, 0.1133, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6211, 0.9922, 0.4688,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.6367, 0.9922, 0.8438, 0.0625, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6211, 0.9922,\n",
       "          0.2617, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0547, 0.3359, 0.6953, 0.9688, 0.9922, 0.3555, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6211,\n",
       "          0.9922, 0.3320, 0.0000, 0.0000, 0.0000, 0.1836, 0.1914, 0.4531, 0.5625,\n",
       "          0.5859, 0.9414, 0.9492, 0.9141, 0.6992, 0.9414, 0.9844, 0.1562, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.5859, 0.9883, 0.9258, 0.8086, 0.8086, 0.8086, 0.9883, 0.9922, 0.9766,\n",
       "          0.9375, 0.7734, 0.5586, 0.3555, 0.1094, 0.0195, 0.9102, 0.9766, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.4648, 0.6914, 0.6914, 0.6914, 0.6914, 0.6914, 0.3828,\n",
       "          0.2188, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3984, 0.9922, 0.8594,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6602, 0.9922,\n",
       "          0.5352, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6602,\n",
       "          0.9922, 0.2227, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.6602, 0.9922, 0.2227, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.6602, 0.9961, 0.3672, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.6602, 0.9922, 0.3750, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.6602, 0.9922, 0.5977, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.6602, 0.9961, 0.5977, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3750, 0.9922, 0.5977, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000]), tensor(4))]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "karen = [small_ds[ii] for ii in jim]\n",
    "karen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "janice = small_ds[jim];\n",
    "cc,dd =janice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa,bb = zip(*karen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's check to see that I don't need collate()\n",
    "#by comparing my solution: ds[Sampler(i)]\n",
    "#versus: collate([ds[i] for i in Sampler(i)])\n",
    "torch.all(torch.eq(torch.stack(aa),cc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hey! I found an easier way to implement random sampler!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -->End of Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay let's try to re-do our DataLoader() class using random Sampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler): self.ds, self.sampler = ds,sampler\n",
    "    def __iter__(self):\n",
    "        for ss in self.sampler: yield self.ds[ss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds,bs,shuffle=True)\n",
    "valid_samp = Sampler(valid_ds,bs,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds,train_samp)\n",
    "valid_dl = DataLoader(valid_ds,valid_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAALgUlEQVR4nO3dT4hd9RnG8eep1Y0KTSqm0xiqLe6EagnZKM0EUdJsoguLWZRIpeOiFgtdGOwiE0ohlFbpqhAxGItVBBVDkdYgM5N2I5mENCYGTSqpjgmZSloaV1Z9u7gn5Rrn/sk959xzMu/3A5d77zn3nvNy7jzzO/9/jggBWP6+1HQBAMaDsANJEHYgCcIOJEHYgSS+PM6Z2WbXP1CziPBSw0u17LY32n7b9knb28pMC0C9POpxdttXSHpH0l2SFiQdkLQlIt7q8x1adqBmdbTs6ySdjIh3I+JjSc9L2lxiegBqVCbsqyW93/V+oRj2ObanbM/bni8xLwAlldlBt9SqwhdW0yNil6RdEqvxQJPKtOwLktZ0vb9B0uly5QCoS5mwH5B0s+2bbF8l6X5Je6spC0DVRl6Nj4hPbD8s6c+SrpC0OyKOVVYZgEqNfOhtpJmxzQ7UrpaTagBcPgg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSYzcP7sk2T4l6bykTyV9EhFrqygKQPVKhb2wISI+rGA6AGrEajyQRNmwh6TXbB+0PbXUB2xP2Z63PV9yXgBKcESM/mX76xFx2vb1kvZJ+klE7O/z+dFnBmAoEeGlhpdq2SPidPG8KOllSevKTA9AfUYOu+2rbV974bWkuyUdraowANUqszd+laSXbV+Yzh8i4k+VVIXWmJyc7Dt+Zmam7/gdO3b0HDc9PT1CRRjVyGGPiHclfbvCWgDUiENvQBKEHUiCsANJEHYgCcIOJFHqDLpLnhln0F12xvn3sZzMzs72HT83NzfytAcdsqzlDDoAlw/CDiRB2IEkCDuQBGEHkiDsQBKEHUiiihtO4jI26BLV5WrQcfBBBh0nX79+/cjTLltbL7TsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEx9mXuUHXPg+6VfQgg44J97uVdNlp49LQsgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEhxnRykbNmxougQMaWDLbnu37UXbR7uGrbS9z/aJ4nlFvWUCKGuY1finJW28aNg2Sa9HxM2SXi/eA2ixgWGPiP2Szl00eLOkPcXrPZLuqbguABUbdZt9VUSckaSIOGP7+l4ftD0laWrE+QCoSO076CJil6RdEh07Ak0a9dDbWdsTklQ8L1ZXEoA6jBr2vZK2Fq+3SnqlmnIA1GXgarzt5yRNSrrO9oKk7ZJ2SnrB9oOS3pN0X51FYnRl7l8ucRx9ORkY9ojY0mPUnRXXAqBGnC4LJEHYgSQIO5AEYQeSIOxAEo4Y30ltnEE3fmV/X9sVVYJxiYglfzRadiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCLpvR1/T0dKnxaA9adiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgvvGL3N1/77cV759Rr5vvO3dthdtH+0aNm37A9uHi8emKosFUL1hVuOflrRxieFPRMStxePVassCULWBYY+I/ZLOjaEWADUqs4PuYdtHitX8Fb0+ZHvK9rzt+RLzAlDSUDvobN8o6Y8RcUvxfpWkDyWFpF9ImoiIHw4xHXbQjRk76PKptGPHiDgbEZ9GxGeSnpS0rkxxAOo3UthtT3S9vVfS0V6fBdAOA69nt/2cpElJ19lekLRd0qTtW9VZjT8l6aEaa0SDZmdnmy4BFRkY9ojYssTgp2qoBUCNOF0WSIKwA0kQdiAJwg4kQdiBJLiVNPqam5trugRUhJYdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5LgOPsyMDk5OfJ3d+zY0Xc8XTIvH7TsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEx9mXge3btzddAi4DtOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kIQjYnwzs8c3s0TK/Ia2K6wEbRARS/6oA1t222tsz9g+bvuY7UeK4Stt77N9onheUXXRAKozsGW3PSFpIiIO2b5W0kFJ90h6QNK5iNhpe5ukFRHx6IBp0bLXgJYd3UZu2SPiTEQcKl6fl3Rc0mpJmyXtKT62R51/AABa6pLOjbd9o6TbJL0haVVEnJE6/xBsX9/jO1OSpsqVCaCsoXfQ2b5G0pykX0bES7b/HRFf6Rr/r4jou93Oanw9WI1Ht5FX4yXJ9pWSXpT0bES8VAw+W2zPX9iuX6yiUAD1GLga786//qckHY+Ix7tG7ZW0VdLO4vmVWipEqds5D7pVNPIYZpv9dkk/kPSm7cPFsMfUCfkLth+U9J6k++opEUAVBoY9Iv4qqdeG3Z3VlgOgLpwuCyRB2IEkCDuQBGEHkiDsQBLcSroFBnW5zK2iUQVadiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgltJV2BmZqbv+Lm5ub7j169f33f8oOPw/XAnmnxK3akGwOWPsANJEHYgCcIOJEHYgSQIO5AEYQeS4Hr2Maj7evQNGzbUOn0sD7TsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5DEwOvZba+R9Iykr0n6TNKuiPit7WlJP5L0z+Kjj0XEqwOmtSyvZx90vXmZ69ElaXZ2ttR45NLrevZhTqr5RNLPIuKQ7WslHbS9rxj3RET8uqoiAdRnmP7Zz0g6U7w+b/u4pNV1FwagWpe0zW77Rkm3SXqjGPSw7SO2d9te0eM7U7bnbc+XqhRAKUOH3fY1kl6U9NOI+I+k30n6lqRb1Wn5f7PU9yJiV0SsjYi1FdQLYERDhd32leoE/dmIeEmSIuJsRHwaEZ9JelLSuvrKBFDWwLC7c3vSpyQdj4jHu4ZPdH3sXklHqy8PQFWGOfR2h6S/SHpTnUNvkvSYpC3qrMKHpFOSHip25vWb1rI89Aa0Sa9Db9w3HlhmuG88kBxhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgiXF32fyhpH90vb+uGNZGba2trXVJ1DaqKmv7Rq8RY72e/Qszt+fbem+6ttbW1rokahvVuGpjNR5IgrADSTQd9l0Nz7+fttbW1rokahvVWGprdJsdwPg03bIDGBPCDiTRSNhtb7T9tu2Ttrc1UUMvtk/ZftP24ab7pyv60Fu0fbRr2Erb+2yfKJ6X7GOvodqmbX9QLLvDtjc1VNsa2zO2j9s+ZvuRYnijy65PXWNZbmPfZrd9haR3JN0laUHSAUlbIuKtsRbSg+1TktZGROMnYNj+rqSPJD0TEbcUw34l6VxE7Cz+Ua6IiEdbUtu0pI+a7sa76K1oorubcUn3SHpADS67PnV9X2NYbk207OsknYyIdyPiY0nPS9rcQB2tFxH7JZ27aPBmSXuK13vU+WMZux61tUJEnImIQ8Xr85IudDPe6LLrU9dYNBH21ZLe73q/oHb19x6SXrN90PZU08UsYdWFbraK5+sbrudiA7vxHqeLuhlvzbIbpfvzspoI+1Jd07Tp+N/tEfEdSd+T9ONidRXDGaob73FZopvxVhi1+/Oymgj7gqQ1Xe9vkHS6gTqWFBGni+dFSS+rfV1Rn73Qg27xvNhwPf/Xpm68l+pmXC1Ydk12f95E2A9Iutn2TbavknS/pL0N1PEFtq8udpzI9tWS7lb7uqLeK2lr8XqrpFcarOVz2tKNd69uxtXwsmu8+/OIGPtD0iZ19sj/XdLPm6ihR13flPS34nGs6dokPafOat1/1VkjelDSVyW9LulE8byyRbX9Xp2uvY+oE6yJhmq7Q51NwyOSDhePTU0vuz51jWW5cboskARn0AFJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEv8DKUXN/n+ywHcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey I got different results for each xb[0]! coolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3010, grad_fn=<NllLossBackward>), tensor(0.8438))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "fit()\n",
    "\n",
    "loss,acc = loss_func(model(xb),yb),accuracy(model(xb),yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    xs,ys = zip(*b)\n",
    "    return torch.stack(xs),torch.stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds,bs,sampler=RandomSampler(train_ds),collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds,bs,sampler=SequentialSampler(valid_ds),collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0988, grad_fn=<NllLossBackward>), tensor(0.9688))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "fit()\n",
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds,bs,shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds,bs,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1415, grad_fn=<NllLossBackward>), tensor(0.9531))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "fit()\n",
    "loss_func(model(xb),yb),accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so now we want to have a validation run through without optimization..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is our most up-to-date fit() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for i in range(epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred,yb)\n",
    "    #backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our fit function take in arguments for all the stuff it needs so it isn't dependent on things that are set in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, train_dl, valid_dl, model, loss_func, aopt):\n",
    "    for i in range(epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred,yb)\n",
    "    #backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay next we are going to try to figure out what our validation dataset should be doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be honest, I think it is a bit weird to calculate loss on the validation set AS I am training vs AFTER I  am training...I mean I guess it is a way to stop our learning if the model is overfitting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, train_dl, valid_dl, model, loss_func, opt, accuracy):\n",
    "    for i in range(epochs): #number of iterations of training and validation metrics calculated        \n",
    "        #training forward pass\n",
    "        for xb,yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred,yb)\n",
    "        #backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        #validation forward pass   \n",
    "        with torch.no_grad():\n",
    "            total_loss, total_acc = 0.,0.\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                total_loss += loss_func(pred,yb)\n",
    "                total_acc  += accuracy(pred,yb)\n",
    "            nb = len(valid_dl) #number of equally-sized mini-batches\n",
    "            print(f\"epoch: {i} | valid_loss: {total_loss / nb} | valid_acc: {total_acc / nb}\")\n",
    "    return total_loss/nb, total_acc/nb        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeremy wants to be fancy and make a function to take in train_ds, valid_ds and pop our dataloaders out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dls(train_ds, valid_ds, bs,**kwargs):\n",
    "    return DataLoader(train_ds, batch_size=bs,shuffle=True,**kwargs), DataLoader(valid_ds, batch_size=2*bs,shuffle=False,**kwargs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | valid_loss: 0.1429130584001541 | valid_acc: 0.9587618708610535\n",
      "epoch: 1 | valid_loss: 0.11745645105838776 | valid_acc: 0.9657832384109497\n",
      "epoch: 2 | valid_loss: 0.12238829582929611 | valid_acc: 0.9633108973503113\n",
      "epoch: 3 | valid_loss: 0.4296751320362091 | valid_acc: 0.8893393874168396\n",
      "epoch: 4 | valid_loss: 0.17806768417358398 | valid_acc: 0.9476858973503113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.1781), tensor(0.9477))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl,valid_dl = get_dls(train_ds,valid_ds,bs)\n",
    "model,opt = get_model()\n",
    "loss,acc = fit(5,train_dl,valid_dl,model,loss_func,opt,accuracy)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc>0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hey I did it!!! I made a model, fit it, and confirmed it using validation data!! Whoohooo....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 03_minibatch_training_OMO.ipynb to exp/nb_03_OMO.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 03_minibatch_training_OMO.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
